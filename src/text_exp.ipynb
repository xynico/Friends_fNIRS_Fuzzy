{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON=/projects/CIBCIGroup/00DataUploading/Xiaowei/anaconda/dl_pl/dl_pl/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "from shallowmind.src.model import ModelInterface\n",
    "from shallowmind.src.data import DataInterface\n",
    "from shallowmind.api.infer import prepare_inference\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "pl.seed_everything(42)\n",
    "from einops import rearrange\n",
    "import os\n",
    "import numpy as np\n",
    "from umap.umap_ import UMAP\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir='/projects/CIBCIGroup/00DataUploading/Liang/fuzzy_fnirs/Friends_fNIRS_Fuzzy/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "ckpt = '/projects/CIBCIGroup/00DataUploading/Liang/fuzzy_fnirs/best_models/fuzzy/FTF-ds=PictureRating_l-d=3-nh=11-r=1-bs=256-ls=0.00015-label=hand/ckpts/exp_name=FTF-ds=PictureRating_l-d=3-nh=11-r=1-bs=256-ls=0.00015-label=hand-cfg=FuzzyTramsformer_num_rules1_num_heads11_dataset_name=PictureRating_label=hand_base_lr=1.5e-4_depth=3_batch_size=256-bs=256-seed=42-val_f1_score=0.6537.ckpt'\n",
    "config = 'best_models/fuzzy/FTF-ds=PictureRating_l-d=3-nh=11-r=1-bs=256-ls=0.00015-label=hand/FuzzyTramsformer_num_rules1_num_heads11_dataset_name=PictureRating_label=hand_base_lr=1.5e-4_depth=3_batch_size=256.py'\n",
    "\n",
    "\n",
    "\n",
    "data_module, model = prepare_inference(config, ckpt)\n",
    "data_module.setup()\n",
    "\n",
    "# 1. draw centers\n",
    "# 2. Friend Attention statistics\n",
    "\n",
    "\n",
    "centers=model.model.encoder.encoder.layers[0].self_attn.rules_keys\n",
    "\n",
    "centers=rearrange(centers, 'h r d -> r (h d)')\n",
    "test_loader = data_module.test_dataloader()\n",
    "data_table = test_loader.dataset.data_table\n",
    "model = model.eval()\n",
    "\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "val_loader = data_module.val_dataloader()\n",
    "data_table = val_loader.dataset.data_table\n",
    "res = pd.DataFrame(columns=['pred', 'label'])\n",
    "\n",
    "embs = []\n",
    "data_info = test_loader.dataset.data_index_table\n",
    "all_data=[]\n",
    "for batch_idx, d in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "    torch.cuda.empty_cache()\n",
    "    data = {'seq': d[0]['seq'].to(device),}\n",
    "    # all_data.append(data['seq'])\n",
    "    label = d[1].to(device)\n",
    "    with torch.no_grad():\n",
    "        imgs = model.model.gen_data(data)\n",
    "        # print(model.model.encoder.encoder.layers[0].self_attn)\n",
    "        output,query= model.model.encoder.encoder.layers[0].self_attn(imgs,imgs,imgs,return_query=True)\n",
    "        all_data.append(query)\n",
    "        latent = model.model.forward_encoder(imgs)\n",
    "        \n",
    "        latent = torch.cat([latent[:latent.shape[0]//2, :], latent[latent.shape[0]//2:, :]], dim=1)\n",
    "        embs.append(latent.cpu().numpy())\n",
    "        latent = model.model.cls(latent)\n",
    "        pred = latent.squeeze(1)\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        pred = pred.cpu().numpy()\n",
    "        label = label.cpu().numpy()\n",
    "        res = pd.concat([res, pd.DataFrame({'pred': pred, 'label': label})], axis=0)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
