{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pipeline is not setup, will use identity transform\n",
      "The pipeline is not setup, will use identity transform\n",
      "The pipeline is not setup, will use identity transform\n",
      "The pipeline is not setup, will use identity transform\n"
     ]
    }
   ],
   "source": [
    "from shallowmind.src.model import ModelInterface\n",
    "from shallowmind.src.data import DataInterface\n",
    "from shallowmind.api.infer import prepare_inference\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pytorch_lightning as pl\n",
    "pl.seed_everything(42)\n",
    "# hand, d=2\n",
    "# ckpt = '/data/xiaowjia/Friends_fNIRS/work_dir/CL-S2-Tf-d=2-nh=11bs=256-ls=0.00015-CL_way=pair-label=hand/ckpts/exp_name=CL-S2-Tf-d=2-nh=11bs=256-ls=0.00015-CL_way=pair-label=hand-cfg=fNIRS_Emo_tramsformer-CL_V0_step2-bs=256-seed=42-val_loss=0.2972.ckpt'\n",
    "# config = '/data/xiaowjia/Friends_fNIRS/work_dir/CL-S2-Tf-d=2-nh=11bs=256-ls=0.00015-CL_way=pair-label=hand/fNIRS_Emo_tramsformer-CL_V0_step2.py'\n",
    "\n",
    "# hand, d=4\n",
    "# ckpt = '/data/xiaowjia/Friends_fNIRS/work_dir/CL-S2-Tf-d=4-nh=11bs=128-ls=0.00015-CL_way=pair-label=hand/ckpts/exp_name=CL-S2-Tf-d=4-nh=11bs=128-ls=0.00015-CL_way=pair-label=hand-cfg=fNIRS_Emo_tramsformer-CL_V0_step2-bs=128-seed=42-val_loss=0.2718.ckpt'\n",
    "# config = '/data/xiaowjia/Friends_fNIRS/work_dir/CL-S2-Tf-d=4-nh=11bs=128-ls=0.00015-CL_way=pair-label=hand/fNIRS_Emo_tramsformer-CL_V0_step2.py'\n",
    "\n",
    "# relationship, d=2\n",
    "# ckpt = '/data/xiaowjia/Friends_fNIRS/work_dir/CL-S2-Tf-d=2-nh=11bs=128-ls=0.00015-CL_way=pair-label=relationship/ckpts/exp_name=CL-S2-Tf-d=2-nh=11bs=128-ls=0.00015-CL_way=pair-label=relationship-cfg=fNIRS_Emo_tramsformer-CL_V0_step2-bs=128-seed=42-val_loss=0.0696.ckpt'\n",
    "# config = '/data/xiaowjia/Friends_fNIRS/work_dir/CL-S2-Tf-d=2-nh=11bs=128-ls=0.00015-CL_way=pair-label=relationship/fNIRS_Emo_tramsformer-CL_V0_step2.py'\n",
    "\n",
    "# # relationship, 0.1, d=2\n",
    "# ckpt ='/home/xiaowjia/data/Friends_fNIRS/work_dir/CL-S2-Tf-ts=0.1-d=2-nh=11bs=256-ls=0.00015-CL_way=pair-label=relationship/ckpts/exp_name=CL-S2-Tf-ts=0.1-d=2-nh=11bs=256-ls=0.00015-CL_way=pair-label=relationship-cfg=fNIRS_Emo_tramsformer-CL_V0_step2-bs=256-seed=42-val_loss=0.0082.ckpt'\n",
    "# config = '/home/xiaowjia/data/Friends_fNIRS/work_dir/CL-S2-Tf-ts=0.1-d=2-nh=11bs=256-ls=0.00015-CL_way=pair-label=relationship/fNIRS_Emo_tramsformer-CL_V0_step2.py'\n",
    "\n",
    "# hand, 0.1, d=2\n",
    "# ckpt = '/data/xiaowjia/Friends_fNIRS/work_dir/CL-S2-Tf-ts=0.1-d=2-nh=11bs=256-ls=0.00015-CL_way=pair-label=hand/ckpts/exp_name=CL-S2-Tf-ts=0.1-d=2-nh=11bs=256-ls=0.00015-CL_way=pair-label=hand-cfg=fNIRS_Emo_tramsformer-CL_V0_step2-bs=256-seed=42-val_loss=0.0733.ckpt'\n",
    "# config = '/home/xiaowjia/data/Friends_fNIRS/work_dir/CL-S2-Tf-ts=0.1-d=2-nh=11bs=256-ls=0.00015-CL_way=pair-label=hand/fNIRS_Emo_tramsformer-CL_V0_step2.py'\n",
    "\n",
    "# AllData relationship, d=2\n",
    "ckpt = '/data/xiaowjia/Friends_fNIRS/work_dir/S2_AllData-Tf-ts=0.3-d=2-nh=11bs=128-ls=0.00015-label=relationship/ckpts/exp_name=S2_AllData-Tf-ts=0.3-d=2-nh=11bs=128-ls=0.00015-label=relationship-cfg=fNIRS_Emo_tramsformer-AllData_V0_step2-bs=128-seed=42-val_f1_score=0.8723.ckpt'\n",
    "config = '/home/xiaowjia/data/Friends_fNIRS/work_dir/S2_AllData-Tf-ts=0.3-d=2-nh=11bs=128-ls=0.00015-label=relationship/fNIRS_Emo_tramsformer-AllData_V0_step2.py'\n",
    "\n",
    "data_module, model = prepare_inference(config, ckpt)\n",
    "data_module.setup()\n",
    "test_loader = data_module.test_dataloader()\n",
    "data_table = test_loader.dataset.data_table\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    # This is a wrapper around the model that we want to explain\n",
    "\n",
    "    def __init__(self, model_interface: ModelInterface):\n",
    "        super(Net, self).__init__()\n",
    "        self.model = model_interface.model\n",
    "        self.model.eval()\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        # Avoid in-place operations by using .clone() and ensure that operations\n",
    "        # are not modifying tensors that are part of the computational graph.\n",
    "\n",
    "        # Clone imgs to avoid modifying the original tensor\n",
    "        imgs_cloned = imgs.clone()\n",
    "\n",
    "        # Concatenating the images\n",
    "        imgs_concat = torch.cat([imgs_cloned[:, 0, :, :], imgs_cloned[:, 1, :, :]], dim=0)\n",
    "\n",
    "        # Forward pass through the encoder\n",
    "        latent = self.model.forward_encoder(imgs_concat)\n",
    "\n",
    "        # Clone latent to avoid in-place operations\n",
    "        latent_cloned = latent.clone()\n",
    "\n",
    "        # Concatenating latent representations\n",
    "        half_size = latent_cloned.shape[0] // 2\n",
    "        latent_concat = torch.cat([latent_cloned[:half_size, :], latent_cloned[half_size:, :]], dim=1)\n",
    "\n",
    "        # Passing through the classifier\n",
    "        pred = self.model.cls(latent_concat).squeeze(1)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:00<00:00, 208.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# SHAP analysis\n",
    "# device = torch.device('cuda:0')\n",
    "# model = model.to(device)\n",
    "data_table = test_loader.dataset.data_table\n",
    "data_info = test_loader.dataset.data_index_table.reset_index(drop=True)\n",
    "data_info['idx'] = data_info.index\n",
    "data = {'seq': [],}\n",
    "label = []\n",
    "for batch_idx, d in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "    torch.cuda.empty_cache()\n",
    "    data['seq'].append(d[0]['seq'])\n",
    "    label.append(d[1])\n",
    "data['seq'] = torch.cat(data['seq'], dim=0)\n",
    "label = torch.cat(label, dim=0)\n",
    "imgs = data['seq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 7) (16942, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_info_test, data_info_bg = train_test_split(data_info, test_size=0.005, random_state=42, stratify=data_info[['relationship_label', 'hand', 'emotion_label']])\n",
    "data_info_bg = data_info_bg.reset_index(drop=True)\n",
    "data_info_test = data_info_test.reset_index(drop=True)\n",
    "print(data_info_bg.shape, data_info_test.shape)\n",
    "\n",
    "imgs_bg = data['seq'][data_info_bg['idx'].values]\n",
    "imgs_test = data['seq'][data_info_test['idx'].values][:3]\n",
    "label_bg = label[data_info_bg['idx'].values]\n",
    "label_test = label[data_info_test['idx'].values][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The SHAP explanations do not sum up to the model's output! This is either because of a rounding error or because an operator in your computation graph was not fully supported. If the sum difference of %f is significant compared to the scale of your model outputs, please post as a github issue, with a reproducible example so we can debug it. Used framework: pytorch - Max. diff: 11.089337036464258 - Tolerance: 0.01",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m e \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mDeepExplainer(Net(model)\u001b[38;5;241m.\u001b[39mto(device), imgs_bg\u001b[38;5;241m.\u001b[39mto(device), torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/xiaowjia/miniconda3/envs/dl_pl/lib/python3.10/site-packages/shap/explainers/_deep/__init__.py:125\u001b[0m, in \u001b[0;36mDeepExplainer.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, ranked_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, check_additivity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     92\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m        were chosen as \"top\".\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranked_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rank_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_additivity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_additivity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/xiaowjia/miniconda3/envs/dl_pl/lib/python3.10/site-packages/shap/explainers/_deep/deep_pytorch.py:219\u001b[0m, in \u001b[0;36mPyTorchDeep.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    217\u001b[0m             model_output_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39mX)\n\u001b[0;32m--> 219\u001b[0m     \u001b[43m_check_additivity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_output_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_phis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_output:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_phis[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/data/xiaowjia/miniconda3/envs/dl_pl/lib/python3.10/site-packages/shap/explainers/_deep/deep_utils.py:20\u001b[0m, in \u001b[0;36m_check_additivity\u001b[0;34m(explainer, model_output_values, output_phis)\u001b[0m\n\u001b[1;32m     16\u001b[0m         diffs \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m output_phis[t][i]\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, output_phis[t][i]\u001b[38;5;241m.\u001b[39mndim)))\n\u001b[1;32m     18\u001b[0m maxdiff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(diffs)\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m maxdiff \u001b[38;5;241m<\u001b[39m TOLERANCE, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe SHAP explanations do not sum up to the model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms output! This is either because of a \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     21\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrounding error or because an operator in your computation graph was not fully supported. If \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     22\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe sum difference of \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m is significant compared to the scale of your model outputs, please post \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     23\u001b[0m                             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas a github issue, with a reproducible example so we can debug it. Used framework: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexplainer\u001b[38;5;241m.\u001b[39mframework\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Max. diff: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmaxdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Tolerance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTOLERANCE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The SHAP explanations do not sum up to the model's output! This is either because of a rounding error or because an operator in your computation graph was not fully supported. If the sum difference of %f is significant compared to the scale of your model outputs, please post as a github issue, with a reproducible example so we can debug it. Used framework: pytorch - Max. diff: 11.089337036464258 - Tolerance: 0.01"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "e = shap.DeepExplainer(Net(model).to(device), imgs_bg.to(device), torch.device('cuda:0'))\n",
    "shap_values = e.shap_values(imgs_test.to(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
